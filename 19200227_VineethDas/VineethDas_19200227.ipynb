{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all the required libraries...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "# import nltk\n",
    "# nltk.download('words')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Selecting three categories and scrapping through the website to create a dataset...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assigning the 3 URLs required...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1_auto = 'http://mlg.ucd.ie/modules/yalp/cafes_list.html'\n",
    "url2_hotels = 'http://mlg.ucd.ie/modules/yalp/hotels_list.html'\n",
    "url3_fashion = 'http://mlg.ucd.ie/modules/yalp/restaurants_list.html'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to fetch the data from the site.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to fetch the data from the website and store, clean it and store it in proper columns - 'Text' and 'Rating'.... \n",
    "\n",
    "def populate_dataframe(url):\n",
    "    \n",
    "    response = get(url)\n",
    "    html_soup = BeautifulSoup(response.text,'html.parser')\n",
    "    #to fetch all the links \n",
    "    links = html_soup.find_all('h5')\n",
    "    # storing the link's pre details in a seperate variable....\n",
    "    link_pre = 'http://mlg.ucd.ie/modules/yalp/'\n",
    "    \n",
    "    \n",
    "    for i in range(len(links)):\n",
    "        if i == 0: # to define the data frame for the first time\n",
    "            data_set1 = pd.DataFrame(columns=['Rating'])\n",
    "            data_set2 = pd.DataFrame(columns=['Text'])\n",
    "            \n",
    "        url = link_pre+(links[i].find('a')['href']) # to store the individual link in the url that can be used to call and fetch the related data\n",
    "        #Fetching the data from the website and parsing the Beautiful_soup function through it\n",
    "        response = get(url)\n",
    "        html_soup = BeautifulSoup(response.text,'html.parser')\n",
    "        # finding just the 'rating' and 'review' from the entire parsed webpage\n",
    "        rating = html_soup.find_all(class_='rating')\n",
    "        review = html_soup.find_all(class_='review-text')\n",
    "        # Loop to iterate through the webpage\n",
    "        for x in range(len(review)):\n",
    "            data_set2 = data_set2.append({'Text':review[x].text},ignore_index=True)# to append only the text value in the dataset\n",
    "            rate = (rating[x].find('img')['alt'])# to get only ratings\n",
    "            data_set1 = data_set1.append({'Rating':rate},ignore_index=True) # to store only the ratings\n",
    "    \n",
    "    data_set1['Text'] = data_set2\n",
    "    return data_set1 \n",
    "    \n",
    "            \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling the function...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling function to populate th data frames\n",
    "\n",
    "data_auto = populate_dataframe(url1_auto)\n",
    "data_fash = populate_dataframe(url2_hotels)\n",
    "data_hotel = populate_dataframe(url3_fashion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a lable column to assign the overall sentiment for the dataset...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding one column to store the lable \"positive\" and \"negative\"\n",
    "\n",
    "def rating_label(rating):\n",
    "    if (rating == '3-star')or(rating=='2-star')or(rating=='1-star'):\n",
    "        return \"negative\"\n",
    "    else :\n",
    "        return \"positive\"\n",
    "\n",
    "data_auto['class_label'] = data_auto.apply(lambda i:rating_label(i['Rating']),axis=1)\n",
    "data_fash['class_label'] = data_fash.apply(lambda i:rating_label(i['Rating']),axis=1)\n",
    "data_hotel['class_label'] = data_hotel.apply(lambda i:rating_label(i['Rating']),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing and applying the TFIDF vectoriser to each of the dataset, fitting the dataset to a model and evaluating its performance...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for preprocessing the data/ cleaning the data...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting of the preprocessing steps for the dataframes\n",
    "\n",
    "# Lower Case  the text elements to ensure uniformity\n",
    "\n",
    "\n",
    "def lower_case_data(rating):\n",
    "    new_text = \"\"\n",
    "    for i in rating:\n",
    "        new_text = new_text + \"\" + i.lower()\n",
    "    return new_text\n",
    "\n",
    "\n",
    "# Removing the stop words as they don't provide any additional value to the dataset\n",
    "stpwrd = stopwords.words('english')\n",
    "\n",
    "def remove_stop_words(rating):\n",
    "    rating = word_tokenize(rating)\n",
    "    new_text = \"\"\n",
    "    for i in rating:\n",
    "        if i not in stpwrd:\n",
    "            new_text = new_text + \" \" + i\n",
    "        \n",
    "    return new_text    \n",
    "\n",
    "# Removing the unnessary symbols from the dataset\n",
    "symbols = \"!\\\"'#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "\n",
    "def remove_punct(rating):\n",
    "    new_text = \"\"\n",
    "    for i in rating:\n",
    "        if i not in symbols:\n",
    "            new_text = new_text + i\n",
    "        #rating = np.char.replace(rating,i,' ')\n",
    "    return new_text\n",
    "\n",
    "######### Can add logic to remove the Apostrophy\n",
    "\n",
    "# Removing numbers from the dataset\n",
    "numbers = \"0123456789\"\n",
    "\n",
    "def remove_num(rating):\n",
    "    new_text = \"\"\n",
    "    for i in rating:\n",
    "        if i not in numbers:\n",
    "            new_text = new_text + i\n",
    "    return new_text\n",
    "\n",
    "\n",
    "# Removing Single Character from the dataset\n",
    "\n",
    "def remove_single_chr(rating):\n",
    "    rating = word_tokenize(rating)\n",
    "    new_text = \"\"\n",
    "    for i in rating:\n",
    "        if len(i) > 1:\n",
    "            new_text = new_text + \" \" + i\n",
    "    return new_text \n",
    "\n",
    "# Stemming the words to its verb or noun word (Stemming) returns word that can/cannot be in the dictionary\n",
    "# If using lemmatization, use it first then use stemmers\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stemming_data(rating):\n",
    "    new_text = \"\"\n",
    "    rating = word_tokenize(rating)\n",
    "    for i in rating:\n",
    "        new_text = new_text + \" \" + ps.stem(i)\n",
    "    return new_text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling a single function to complete all the other preprossesing steps in one go...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definig one function to carry out all sub-function operations\n",
    "def preproccesing_data(dataframe):\n",
    "    dataframe['Text'] = dataframe.apply(lambda i:lower_case_data(i['Text']),axis=1)\n",
    "    dataframe['Text'] = dataframe.apply(lambda i:remove_punct(i['Text']),axis=1)\n",
    "    dataframe['Text'] = dataframe.apply(lambda i:remove_num(i['Text']),axis=1)\n",
    "    dataframe['Text'] = dataframe.apply(lambda i:remove_single_chr(i['Text']),axis=1)\n",
    "   # dataframe['Text'] = dataframe.apply(lambda i:remove_non_eng(i['Text']),axis=1)\n",
    "    dataframe['Text'] = dataframe.apply(lambda i:remove_stop_words(i['Text']),axis=1)\n",
    "    dataframe['Text'] = dataframe.apply(lambda i:stemming_data(i['Text']),axis=1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproccesing_data(data_auto)\n",
    "preproccesing_data(data_fash)\n",
    "preproccesing_data(data_hotel)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Text</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4-star</td>\n",
       "      <td>pro lot item would expect chines bakeri frien...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4-star</td>\n",
       "      <td>best eggtart town there realli much say hong ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2-star</td>\n",
       "      <td>ive abc bakeri time sinc read one top place e...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-star</td>\n",
       "      <td>fyi close monday new ownership week new menu ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4-star</td>\n",
       "      <td>insid may look like much make mean egg tart g...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating                                               Text class_label\n",
       "0  4-star   pro lot item would expect chines bakeri frien...    positive\n",
       "1  4-star   best eggtart town there realli much say hong ...    positive\n",
       "2  2-star   ive abc bakeri time sinc read one top place e...    negative\n",
       "3  1-star   fyi close monday new ownership week new menu ...    negative\n",
       "4  4-star   insid may look like much make mean egg tart g...    positive"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to validate....\n",
    "data_auto.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the SkLearn method...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an instance of the classifiers...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Classifiers\n",
    "\n",
    "log = LogisticRegression(solver='lbfgs')\n",
    "ran = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a function to vectorise the dataset...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying the sklearn method\n",
    "\n",
    "def vectorizer(dataset):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform(dataset['Text'])\n",
    "    \n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "    dataset_auto_vectorised = pd.DataFrame(denselist, columns=feature_names)\n",
    "    return dataset_auto_vectorised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calling the vectorisere function...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_auto_vec = vectorizer(data_auto)\n",
    "data_fash_vec = vectorizer(data_fash)\n",
    "data_hotel_vec = vectorizer(data_hotel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to filter, predict and evaluate results in one function...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitter_predict(dataset,original_dataset):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset,original_dataset['class_label'],test_size=0.30)\n",
    "\n",
    "    log.fit(X_train,y_train)\n",
    "\n",
    "    y4_pred = log.predict(X_test)\n",
    "\n",
    "    print(\"Accuracy log:\",metrics.accuracy_score(y_test, y4_pred))\n",
    "#     print(\"Accuracy ran:\",metrics.accuracy_score(y_test, y5_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling function to print the results...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Dataset - Cafe\n",
      "Accuracy log: 0.8016666666666666\n",
      "Results for Dataset - Restaurants\n",
      "Accuracy log: 0.8766666666666667\n",
      "Results for Dataset - Hotel\n",
      "Accuracy log: 0.87\n"
     ]
    }
   ],
   "source": [
    "print('Results for Dataset - Cafe')\n",
    "fitter_predict(data_auto_vec,data_auto)\n",
    "print('Results for Dataset - Restaurants')\n",
    "fitter_predict(data_fash_vec,data_fash)\n",
    "print('Results for Dataset - Hotel')\n",
    "fitter_predict(data_hotel_vec,data_hotel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have seen the use of the metric accuracy score for this metric, while using the logistic regression for applying the fitting and predicting model, an accuracy of 82% to 89%, is achieved. While compared with other estimators, this is the highest. The Dataset Restaurant returns the highest accuracy among the three. We have used the metrics.accuracy_score method here to calculate the accuracy of the dataset, it computes the subset accuracy,the set of labels predicted for a sample must exactly match the corresponding set of labels, all the correct predictors are given as the ration of the total predictons made..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluating the performance of each of the three classificaions when called on each of the dataset...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a function to apply the fit and transform methods on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The overall result will be less as compared to the individual dataset as the vectorized datasets have irregular number of columns (three of them will have different number of columns) and when we try to fit and transform, all the extra data will be lost, leading to reduction in the accuracy score.\n",
    "#### As mentioned, we have selected data from similar backgrounds - Restaurants, hotels and Cafes, hence we can expect better scores than what we could have got if we choose dis-similar datasets like automobile, Gym and hotel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_transform_predict_evaluate(dataset_fit,dataset_trans1,dataset_trans2):\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectors = vectorizer.fit(dataset_fit['Text'])\n",
    "    trans_a = vectors.transform(dataset_fit['Text'])\n",
    "    trans_b = vectors.transform(dataset_trans1['Text'])\n",
    "    trans_c = vectors.transform(dataset_trans2['Text'])\n",
    "    # to create a new dataset using the reduced/increased columns\n",
    "    feature_names_a = vectors.get_feature_names()\n",
    "    dense_a = trans_a.todense()\n",
    "    dense_b = trans_b.todense()\n",
    "    dense_c = trans_c.todense()\n",
    "    # to generate a list item....\n",
    "    denselist_a = dense_a.tolist()\n",
    "    denselist_b = dense_b.tolist()\n",
    "    denselist_c = dense_c.tolist()\n",
    "    # to generate the new datasets....\n",
    "    data_set_train = pd.DataFrame(denselist_a, columns=feature_names_a)\n",
    "    data_set_test1 = pd.DataFrame(denselist_b,columns=feature_names_a)\n",
    "    data_set_test2 = pd.DataFrame(denselist_c,columns=feature_names_a)\n",
    "    \n",
    "    # fitting using log regression....\n",
    "    ran = RandomForestClassifier()\n",
    "    \n",
    "    #Create a Classifiers\n",
    "    \n",
    "    log = LogisticRegression(solver='lbfgs')\n",
    "    \n",
    "    log.fit(data_set_train,dataset_fit['class_label'])\n",
    "    \n",
    "    # predicting the results\n",
    "    y2_pred = log.predict(data_set_test1)\n",
    "    y3_pred = log.predict(data_set_test2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Evaluating the results....\n",
    "    \n",
    "    print(\"Accuracy #1 :\",metrics.accuracy_score(dataset_trans1['class_label'], y2_pred))\n",
    "    print(\"Accuracy #2:\",metrics.accuracy_score(dataset_trans2['class_label'], y3_pred))\n",
    "   \n",
    "\n",
    "   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When choosing the dataset \"Cafe\" to train the model and to predict the dataset #1\"Restaurants\" and #2\"Hotel\" we receive the below Accuracy Scores  \n",
      "Accuracy #1 : 0.8165\n",
      "Accuracy #2: 0.85\n",
      "When choosing the dataset \"Hotel\" to train the model and to predict the dataset #1\"Restaurants\" and #2\"Cafe\" we receive the below Accuracy Scores  \n",
      "Accuracy #1 : 0.817\n",
      "Accuracy #2: 0.86\n",
      "When choosing the dataset \"Restaurants\" to train the model and to predict the dataset #1\"Cafe\" and #2\"Hotel\" we receive the below Accuracy Scores  \n",
      "Accuracy #1 : 0.8325\n",
      "Accuracy #2: 0.816\n"
     ]
    }
   ],
   "source": [
    "print('When choosing the dataset \"Cafe\" to train the model and to predict the dataset #1\"Restaurants\" and #2\"Hotel\" we receive the below Accuracy Scores  ')\n",
    "fit_transform_predict_evaluate(data_auto,data_fash,data_hotel)\n",
    "\n",
    "print('When choosing the dataset \"Hotel\" to train the model and to predict the dataset #1\"Restaurants\" and #2\"Cafe\" we receive the below Accuracy Scores  ')\n",
    "fit_transform_predict_evaluate(data_hotel,data_fash,data_auto)\n",
    "\n",
    "print('When choosing the dataset \"Restaurants\" to train the model and to predict the dataset #1\"Cafe\" and #2\"Hotel\" we receive the below Accuracy Scores  ')\n",
    "fit_transform_predict_evaluate(data_fash,data_auto,data_hotel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As seen from the above scores, we have found an effective model that could train the dataset, and get results upto as high as 86%, even when there are extra words in the dataset, which are erstwhile not present in the training dataset, this happens because we fit and transform the training dataset into the testing dataset, resulting into the same feature names being considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
